{"cells":[{"cell_type":"markdown","metadata":{"id":"Gwyq9meMWugP"},"source":["# Object Detection Federated CNN Server Side\n","This code is the server part of Car Detection federated CNN model for **multi** client and a server."]},{"cell_type":"markdown","metadata":{"id":"Pv_WDKOuWugR"},"source":["## Setting variables"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"lYCYSx40WugS"},"outputs":[],"source":["import os\n","import sys\n","import time\n","import copy\n","import h5py\n","import struct\n","import socket\n","import pickle\n","from tqdm import tqdm\n","\n","import torch\n","import torch.nn as nn\n","from threading import Lock\n","import torch.optim as optim\n","from threading import Thread\n","import torch.nn.functional as F"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"5ThsKjX_WugS"},"outputs":[],"source":["import cv2\n","import ast\n","import math\n","import matplotlib\n","import torchvision\n","import numpy as np\n","import pandas as pd\n","from PIL import Image\n","import os,sys,matplotlib,re\n","from skimage import exposure\n","from tqdm.notebook import tqdm\n","import matplotlib.pyplot as plt\n","import matplotlib.image as immg\n","import torchvision.transforms as T\n","from torchvision.io import read_image\n","from collections import defaultdict, deque\n","import torchvision.transforms as transforms\n","from torch.utils.data import DataLoader, Dataset\n","from torchvision.utils import draw_bounding_boxes\n","from torchvision.models.detection import FasterRCNN\n","from torch.utils.data.sampler import SequentialSampler\n","from torchvision.models.detection.rpn import AnchorGenerator\n","from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"markdown","metadata":{"id":"LPcIoTFbWugS"},"source":["## Declaring Device"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"B3Da0CrmWugT","outputId":"18ca5162-fc12-4ef6-9f3d-8cceb6f8e78d"},"outputs":[{"data":{"text/plain":["device(type='cpu')"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["# Checking if cuda is available \n","#The code will use the GPU if available, otherwise it will use the CPU.\n","device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","device"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"rI4oXqD0WugR"},"outputs":[],"source":["users = 2 # number of clients\n","rounds = 3 # Number of epochs\n","local_epoch = 1"]},{"cell_type":"markdown","metadata":{"id":"vetqwLtjWugT"},"source":["## CNN Model for object Detection"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"6222RWIqWugU"},"outputs":[],"source":["# Defining Pytorch CNN pretrained model\n","#The code attempts to create a model that will predict the class score of an image based on the input image.\n","def model1(num): #it creates a model with the name of \"model1\" and sets its input to be num.\n","    #it assigns this object to variable named model1 so that we can use it later on in our code.\n","    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True) #It then defines a function called \"model\".\n","    #is an instance of torchvision's class called \"DetectionModel\", which contains information about how many features are used for prediction (in this case, 50).\n","    in_features = model.roi_heads.box_predictor.cls_score.in_features #we create another variable called in_features and assign it to be equal to what was just assigned to variable model1 - i.e., 50 features were used for prediction here because there are 50 boxes in each image!\n","    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num) #This function has two parameters: in_features and num.\n","    return model"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"f2LfC7tTWugU"},"outputs":[],"source":["# Creating object of model\n","resnet_cnn = model1(2)"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"1C3NoNcpWugU","outputId":"838fa0b8-e37a-400f-cae4-03b90b6a30b1"},"outputs":[{"data":{"text/plain":["FasterRCNN(\n","  (transform): GeneralizedRCNNTransform(\n","      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n","  )\n","  (backbone): BackboneWithFPN(\n","    (body): IntermediateLayerGetter(\n","      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","      (bn1): FrozenBatchNorm2d(64, eps=0.0)\n","      (relu): ReLU(inplace=True)\n","      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","      (layer1): Sequential(\n","        (0): Bottleneck(\n","          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n","          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n","          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","          (downsample): Sequential(\n","            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","            (1): FrozenBatchNorm2d(256, eps=0.0)\n","          )\n","        )\n","        (1): Bottleneck(\n","          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n","          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n","          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (2): Bottleneck(\n","          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n","          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n","          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","      )\n","      (layer2): Sequential(\n","        (0): Bottleneck(\n","          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n","          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","          (downsample): Sequential(\n","            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","            (1): FrozenBatchNorm2d(512, eps=0.0)\n","          )\n","        )\n","        (1): Bottleneck(\n","          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n","          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (2): Bottleneck(\n","          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n","          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (3): Bottleneck(\n","          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n","          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","      )\n","      (layer3): Sequential(\n","        (0): Bottleneck(\n","          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","          (downsample): Sequential(\n","            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","            (1): FrozenBatchNorm2d(1024, eps=0.0)\n","          )\n","        )\n","        (1): Bottleneck(\n","          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (2): Bottleneck(\n","          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (3): Bottleneck(\n","          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (4): Bottleneck(\n","          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (5): Bottleneck(\n","          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","      )\n","      (layer4): Sequential(\n","        (0): Bottleneck(\n","          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n","          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n","          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","          (downsample): Sequential(\n","            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","            (1): FrozenBatchNorm2d(2048, eps=0.0)\n","          )\n","        )\n","        (1): Bottleneck(\n","          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n","          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n","          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (2): Bottleneck(\n","          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n","          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n","          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","      )\n","    )\n","    (fpn): FeaturePyramidNetwork(\n","      (inner_blocks): ModuleList(\n","        (0): Conv2dNormActivation(\n","          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n","        )\n","        (1): Conv2dNormActivation(\n","          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n","        )\n","        (2): Conv2dNormActivation(\n","          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n","        )\n","        (3): Conv2dNormActivation(\n","          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n","        )\n","      )\n","      (layer_blocks): ModuleList(\n","        (0): Conv2dNormActivation(\n","          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        )\n","        (1): Conv2dNormActivation(\n","          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        )\n","        (2): Conv2dNormActivation(\n","          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        )\n","        (3): Conv2dNormActivation(\n","          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        )\n","      )\n","      (extra_blocks): LastLevelMaxPool()\n","    )\n","  )\n","  (rpn): RegionProposalNetwork(\n","    (anchor_generator): AnchorGenerator()\n","    (head): RPNHead(\n","      (conv): Sequential(\n","        (0): Conv2dNormActivation(\n","          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","          (1): ReLU(inplace=True)\n","        )\n","      )\n","      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n","      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n","    )\n","  )\n","  (roi_heads): RoIHeads(\n","    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n","    (box_head): TwoMLPHead(\n","      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n","      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n","    )\n","    (box_predictor): FastRCNNPredictor(\n","      (cls_score): Linear(in_features=1024, out_features=2, bias=True)\n","      (bbox_pred): Linear(in_features=1024, out_features=8, bias=True)\n","    )\n","  )\n",")"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["# Sending device to device(cpu or gpu)\n","#The code will send the current device's IP address to CNN.\n","resnet_cnn.to(device)"]},{"cell_type":"markdown","metadata":{"id":"zwph-g6pWugU"},"source":["## Variables"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"RfrvZ2PwWugU"},"outputs":[],"source":["# Declaring required Variables\n","# The input list contains three variables: start_time, weightcount, and lock; while the output list contains one variable: datasetsize.\n","#The code attempts to create a copy of the ResNet CNN object, and then use it to initialize the global_weights variable.\n","start_time = 0 #This is used to keep track of the time that has passed since the program started running.\n","weight_count = 0 #It will hold all the users in this dataset.\n","lock = Lock()\n","datasetsize = [0]*users\n","weights_list = [0]*users #It will hold all the weight values for each user.\n","clientsoclist = [0]*users #it creates an array called clientsoclist which will hold all these weight values for each client (or node).\n","\n","# Copying original weights of our model\n","global_weights = copy.deepcopy(resnet_cnn.state_dict())"]},{"cell_type":"markdown","metadata":{"id":"WsuX9a3zWugU"},"source":["## Overhead of Communication"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"GrxiVBrmWugU"},"outputs":[],"source":["# Defining lists for further use\n","total_sendsize_list = [] #creating a list of users (users) and their total sendsize \n","total_receivesize_list = [] #creating a list of users (users) and their total receivesize \n","train_sendsize_list = [] #It creates list  with all of the send sizes that each user has sent\n","train_receivesize_list = [] #It creates list  with all of the Receive sizes that each user has sent\n","client_receivesize_list = [[] for i in range(users)] #it loops through these lists to create a dictionary where keys are users and values are lists containing send size or receive size\n","client_sendsize_list = [[] for i in range(users)] #it loops through these lists to create a dictionary where keys are users and values are lists containing send size or receive size"]},{"cell_type":"markdown","metadata":{"id":"mGHLHtHQWugV"},"source":["### Socket functions"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"-x-TyaLMWugV"},"outputs":[],"source":["# Helper function to receive n bytes or return None if EOF is hit\n","def RECVALL(sock, n): #defining a function called RECVALL.\n","    data = b'' #The code declares the variable data to be of type b'' which is short for \"byte array\".\n","    while len(data) < n: #creates a while loop that will continue until it has received less than n bytes from the socket.\n","        packet = sock.recv(n - len(data)) #reads all the bytes from the socket into packet using recv(n - len(data)).\n","        if not packet: \n","            return None\n","        data += packet #After reading all the bytes into packet, data is set equal to packet + data and continues on with its looping process until it reaches less than n bytes again or when it receives None back from recvall().\n","    return data\n","\n","# Read message length and unpack it into an integer\n","#defines a function that takes in a sock as one parameter and returns either None or some string depending on whether there was any error during sending or receiving packets over this particular connection.\n","def MSG_RECV(sock):\n","    raw_msglen = RECVALL(sock, 4)\n","    if not raw_msglen:\n","        return None\n","    msglen = struct.unpack('>I', raw_msglen)[0]\n","    # Read the message data\n","    msg =  RECVALL(sock, msglen)\n","    msg = pickle.loads(msg)\n","    return msg, msglen\n","\n","# Prefix each message with a 4-byte length in network byte order\n","#another function that sends out strings through sockets based on how many characters are being sent at once (l_send) plus whatever message they want to send (msg).\n","def MSG_SEND(sock, msg):\n","    msg = pickle.dumps(msg)\n","    l_send = len(msg)\n","    msg = struct.pack('>I', l_send) + msg\n","    sock.sendall(msg)\n","    return l_send\n","#The code attempts to send a message of length 4 bytes."]},{"cell_type":"code","execution_count":11,"metadata":{"id":"K_EAR_pYWugV"},"outputs":[],"source":["# Returns the average of the weights\n","#The code iterates through the list of weights, and for each weight it multiplies the value by the corresponding data.\n","#The result is then divided by the sum of all values to get an average.\n","#The code iterates through a list of weights and for each weight it multiplies its value by that weight's corresponding data element.\n","#Then, it divides this product by the sum of all values in order to get an average.\n","def Average_of_Weights(w, datasize):\n","        \n","    for i, data in enumerate(datasize):\n","        for key in w[i].keys():\n","            w[i][key] *= float(data)\n","    \n","    w_avg = copy.deepcopy(w[0])\n","\n","    for key in w_avg.keys():\n","        for i in range(1, len(w)):\n","            w_avg[key] += w[i][key]\n","        w_avg[key] = torch.div(w_avg[key], float(sum(datasize)))\n","        \n","    return w_avg"]},{"cell_type":"markdown","metadata":{"id":"uvmkafIKWugW"},"source":["## Define Thread"]},{"cell_type":"markdown","metadata":{"id":"diynoghQWugW"},"source":["## Receiving Users just before starting Training"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"-B0426NWWugW"},"outputs":[],"source":["# Function to Keep server running \n","def run_thread(func, num_user): ##creating a list of global variables.\n","    global clientsoclist  #will be used to keep track of all the connections that are made during this program.\n","    global start_time #to store when the program started and end_time which stores when it ends.\n","    \n","    thrs = [] #we create an empty list called thrs and then iterate through num_user times adding each connection to clientsoclist\n","    for i in range(num_user):\n","        conn, addr = s.accept()\n","        print('Conntected with', addr)\n","        # append client socket on list\n","        clientsoclist[i] = conn\n","        args = (i, num_user, conn) ##it creates a function that takes three parameters and returns the result of calling that function on each user's connection.\n","        thread = Thread(target=func, args=args) #we create a Thread object where func is the function that needs to run on every thread in thrs.\n","        thrs.append(thread) # we append our new thread into thrs so that it can run on its own after starting it up.\n","        thread.start() #The code attempts to train a threading application with the given number of users.\n","    #The code starts by creating an empty list for the threads, which will be filled in later.\n","    print(\"timmer start!\")\n","    start_time = time.time()    # store start time\n","    for thread in thrs:\n","        thread.join()\n","    end_time = time.time()  # store end time\n","    print(\"TrainingTime: {} sec\".format(end_time - start_time))\n","    #It then loops through each thread in turn and calls its join() method once finished, which causes all threads to exit gracefully."]},{"cell_type":"code","execution_count":13,"metadata":{"id":"mC64A8PvWugW"},"outputs":[],"source":["# Function to receive clients\n","def receive(userid, num_users, conn): #The code starts by creating a dictionary with the keys being userid and rounds.\n","    global weight_count\n","    global datasetsize \n","    msg = {\n","        'rounds': rounds,\n","        'client_id': userid,\n","        'local_epoch': local_epoch\n","    }\n","\n","    # send epoch\n","    datasize = MSG_SEND(conn, msg) \n","    total_sendsize_list.append(datasize) #It will contain all of the sizes for every message sent to every client.\n","    client_sendsize_list[userid].append(datasize) #It will contain all of the sizes for every message sent from each individual client.\n","\n","    # get total_batch of train dataset\n","    train_dataset_size, datasize = MSG_RECV(conn) #locks on datasetsize so that no other process can change its value while this function is running.\n","    #The values are lists of integers that represent how many times each user has been sent messages in this round.  \n","    total_receivesize_list.append(datasize) #It represents how much data has been received from our training dataset\n","    client_receivesize_list[userid].append(datasize) #how much data has been received from our clients (the number of users).\n","    \n","    with lock:\n","        datasetsize[userid] = train_dataset_size\n","        weight_count += 1 #we add 1 to weight count because there is one extra element in both lists now after adding 1 to datasetsize[userid].\n","    \n","    train(userid, train_dataset_size, num_users, conn) # called train() with these new values for num_users and conn so that it can start\n","    #The code attempts to send a message to the server with the following information: rounds: rounds, client_id: userid, local_epoch: local_epoch."]},{"cell_type":"markdown","metadata":{"id":"XBv5YCRbWugW"},"source":["## Train"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"mTcPVqRRWugW"},"outputs":[],"source":["# Function for Training\n","# The train_dataset_size variable is defined as being equal to MSG_SEND(conn, global_weights) which will send out one dataset size worth of data from each client.\n","#The total number of users in this experiment is defined as num_users, so there will be a total number of rounds equal to rounds.\n","\n","def train(userid, train_dataset_size, num_users, client_conn): #defining the variables that are used in the program.\n","    global weights_list\n","    global global_weights #It  is a list of all weights for each user, and it's initialized to an empty list at the beginning of every round.\n","    global weight_count\n","    global resnet_cnn #The resnetcnn classifier was created using Keras and trained on ImageNet with batch normalization and dropout layers.\n","    #It has been pre-trained on ImageNet with over 1 billion images labeled into 1000 categories (1000 classes).\n","    global val_acc\n","    \n","    for r in range(rounds):\n","        with lock: #called lock which will prevent multiple threads from accessing it simultaneously while they're iterating through their respective loops.\n","            if weight_count == num_users: #two nested loops: one for sending messages between clients and another for receiving messages from clients.\n","                for i, conn in enumerate(clientsoclist):\n","                    datasize = MSG_SEND(conn, global_weights)\n","                    total_sendsize_list.append(datasize)\n","                    client_sendsize_list[i].append(datasize)\n","                    train_sendsize_list.append(datasize)\n","                    weight_count = 0\n","    #The code is a snippet of code that is used to train the weights for the neural network.\n","        client_weights, datasize = MSG_RECV(client_conn)\n","        total_receivesize_list.append(datasize)\n","        client_receivesize_list[userid].append(datasize)\n","        train_receivesize_list.append(datasize)\n","\n","        weights_list[userid] = client_weights\n","        print(\"User\" + str(userid) + \"'s Round \" + str(r + 1) +  \" is done\")\n","        with lock:\n","            weight_count += 1\n","            if weight_count == num_users:\n","                #average\n","                global_weights = Average_of_Weights(weights_list, datasetsize)\n","    #The code starts by checking if there are enough users in the dataset.\n","    # If there are not enough users, then it will loop until all users have been processed.\n","    #it will create a list of clients and send them data from the global_weights variable.\n","    #Then, it will check if there are enough clients and repeat this process until all clients have been sent data from the global_weights variable.\n"]},{"cell_type":"markdown","metadata":{"id":"PFHxq3uyWugV"},"source":["## Socket initialization\n","### Set host address and port number"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"rQ-RoB5JWugX","outputId":"344b9906-cd53-43ed-f1bf-90ae26a72029"},"outputs":[{"name":"stdout","output_type":"stream","text":["192.168.43.128\n"]}],"source":["# The code prints the hostname of the machine on which Python is running.\n","host = socket.gethostbyname(socket.gethostname())\n","port = 10087\n","print(host)"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"LzZUK9QWWugX"},"outputs":[],"source":["s = socket.socket() #creating a socket object.\n","s.bind((host, port)) #binds it to the specified host and port.\n","s.listen(5) \n","#This causes the program to wait for incoming connections from other computers (or clients) that are connected to this computer's network interface card (NIC)."]},{"cell_type":"markdown","metadata":{"id":"PZBuxJtoWugX"},"source":["### Open the server socket"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"BBG_flccWugX","outputId":"b50e6c52-192f-4724-a151-3673c1591b2d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Conntected with ('192.168.43.35', 59879)\n","Conntected with ('192.168.43.128', 62174)\n","timmer start!\n"]}],"source":["run_thread(receive, users)\n","#calling receive(), which will be executed in a separate thread of execution.\n","#calling users(), which will also be executed in its own thread of execution.\n","#The code attempts to create a server that listens on port 10087 or whatever port we give."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XPFIZxrDWugX","outputId":"0c5d8c15-00b6-4f21-c05f-5377b7f13f37"},"outputs":[{"name":"stdout","output_type":"stream","text":["TrainingTime: 464.98293137550354 sec\n"]}],"source":["# The code computes the time taken for a training session.\n","end_time = time.time()  # store end time\n","print(\"TrainingTime: {} sec\".format(end_time - start_time))"]},{"cell_type":"markdown","metadata":{"id":"wQtwQfGJax7Z"},"source":["### Saving the model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p1h86sJ_WugX"},"outputs":[],"source":["PATH = './FL_fasterrcnn_car_detect.pth'\n","torch.save(resnet_cnn.state_dict(), PATH)"]},{"cell_type":"markdown","metadata":{"id":"6wVEUicSWugX"},"source":["## Print all of communication overhead"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3IbO5Ip7WugX","outputId":"cf927f71-bcf3-4dc6-dae6-bbc09559ccab"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","---total_sendsize_list---\n","total_sendsize size: 331452604 bytes\n","\n","\n","---total_receivesize_list---\n","total receive sizes: 30 bytes\n","\n","\n","---client_sendsize_list(user0)---\n","total client_sendsizes(user0): 165726302 bytes\n","\n","\n","---client_receivesize_list(user0)---\n","total client_receive sizes(user0): 15 bytes\n","\n","\n","---client_sendsize_list(user1)---\n","total client_sendsizes(user1): 165726302 bytes\n","\n","\n","---client_receivesize_list(user1)---\n","total client_receive sizes(user1): 15 bytes\n","\n","\n","---train_sendsize_list---\n","total train_sendsizes: 331452490 bytes\n","\n","\n","---train_receivesize_list---\n","total train_receivesizes: 0 bytes\n","\n","\n"]}],"source":["#The code starts by printing out the total_sendsize_list and the total_receivesize_list.\n","#The code then prints out a list of all the users in order, followed by their client sendsizes and receivesizes.\n","#The code starts with a for loop that iterates through each user in order to print out their client sendsizes and receivesizes.\n","#The code will print total size of send and received data for each user.\n","\n","print('\\n')\n","print('---total_sendsize_list---')\n","total_size = 0\n","for size in total_sendsize_list:\n","\n","    total_size += size\n","print(\"total_sendsize size: {} bytes\".format(total_size))\n","print('\\n')\n","\n","print('---total_receivesize_list---')\n","total_size = 0\n","for size in total_receivesize_list:\n","\n","    total_size += size\n","print(\"total receive sizes: {} bytes\".format(total_size) )\n","print('\\n')\n","\n","for i in range(users):\n","    print('---client_sendsize_list(user{})---'.format(i))\n","    total_size = 0\n","    for size in client_sendsize_list[i]:\n","\n","        total_size += size\n","    print(\"total client_sendsizes(user{}): {} bytes\".format(i, total_size))\n","    print('\\n')\n","\n","    print('---client_receivesize_list(user{})---'.format(i))\n","    total_size = 0\n","    for size in client_receivesize_list[i]:\n","\n","        total_size += size\n","    print(\"total client_receive sizes(user{}): {} bytes\".format(i, total_size))\n","    print('\\n')\n","\n","print('---train_sendsize_list---')\n","total_size = 0\n","for size in train_sendsize_list:\n","\n","    total_size += size\n","print(\"total train_sendsizes: {} bytes\".format(total_size))\n","print('\\n')\n","\n","print('---train_receivesize_list---')\n","total_size = 0\n","for size in train_receivesize_list:\n","\n","    total_size += size\n","print(\"total train_receivesizes: {} bytes\".format(total_size))\n","print('\\n')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WZg0yW3MWugY"},"outputs":[],"source":["root_path = '../../models/'"]},{"cell_type":"markdown","metadata":{"id":"LDW0UryhWugY"},"source":["## Defining Car Dataset Class\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U2TgBcT4WugY"},"outputs":[],"source":["class CarDataset(object): #The code starts by initializing the CarDataset object with a list of dataframes and an image directory.\n","\n","    def __init__(self, df, IMG_DIR, transforms=None):\n","        self.a = 0\n","        self.df = df\n","        self.img_dir = IMG_DIR\n","        self.image_ids = self.df['img_path'].unique().tolist()\n","        self.transforms = transforms\n","        \n","    def __len__(self):\n","        return len(self.image_ids)\n","        \n","    def __getitem__(self, idx):\n","        image_id = self.image_ids[idx]\n","        records = self.df[self.df['img_path'] == image_id]\n","        image = cv2.imread(self.img_dir+image_id,cv2.IMREAD_COLOR)\n","        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n","        image /= 255.0\n","\n","        boxes = records[['bbox_x1', 'bbox_y1', 'bbox_x2', 'bbox_y2']].to_numpy()\n","        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n","        labels = torch.ones((records.shape[0],), dtype=torch.int64)\n","        \n","        target = {}\n","        target['boxes'] = boxes\n","        target['labels'] = labels\n","        target['image_id'] = torch.tensor([idx])\n","        target['area'] = torch.as_tensor(area, dtype=torch.float32)\n","        target['iscrowd'] = torch.zeros((records.shape[0],), dtype=torch.int64)\n","    \n","        if self.transforms:\n","            sample = {\n","                'image': image,\n","                'bboxes': target['boxes'],\n","                'labels': labels\n","            }\n","            sample = self.transforms(**sample)\n","            image = sample['image']\n","            \n","            target['boxes'] = torch.stack(tuple(map(torch.tensor, zip(*sample['bboxes'])))).permute(1, 0)\n","        return image.clone().detach(), target, image_id"]},{"cell_type":"markdown","metadata":{"id":"Hrwjp8MGWugY"},"source":["## Making Batch Generator"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O4LK9HGrWugY"},"outputs":[],"source":["batch_size = 32"]},{"cell_type":"markdown","metadata":{"id":"BX5tssPCc8H_"},"source":["### Data Augmentation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LKH1CGL1WugZ"},"outputs":[],"source":["import albumentations as A\n","from albumentations.pytorch.transforms import ToTensorV2\n","def train_transform():\n","    return A.Compose([\n","        A.Flip(0.5),\n","        ToTensorV2()\n","    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n","\n","def valid_transform():\n","    return A.Compose([\n","        ToTensorV2()\n","    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3rCWw5EFWugZ"},"outputs":[],"source":["# Dataset path\n","bbox_path = './car_data/images_w_boxes.csv'\n","image_folder_path = './car_data/images'\n","data_bbox = pd.read_csv(bbox_path)\n","df = data_bbox.copy()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TTv-6xStWugZ","outputId":"31faea80-9d95-4d99-b62b-c65067b52397"},"outputs":[{"data":{"text/plain":["((5793, 5), (665, 5))"]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["image_ids = df['img_path'].unique()\n","valid_ids = image_ids[-665:]\n","train_ids = image_ids[:-665]\n","valid_df = df[df['img_path'].isin(valid_ids)]\n","train_df = df[df['img_path'].isin(train_ids)]\n","train_df.shape,valid_df.shape"]},{"cell_type":"markdown","metadata":{"id":"5sgTcLyAWugY"},"source":["### `DataLoader` for batch generating"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8gwG_S5HWugZ"},"outputs":[],"source":["def collate_fn(batch):\n","    return tuple(zip(*batch))\n","\n","train_dataset = CarDataset(train_df, image_folder_path, train_transform())\n","valid_dataset = CarDataset(valid_df, image_folder_path, valid_transform())\n","\n","indices = torch.randperm(len(train_dataset)).tolist()\n","train_data_loader = DataLoader(\n","    train_dataset,\n","    batch_size=batch_size,\n","    shuffle=True,\n","    num_workers=0,\n","    collate_fn=collate_fn\n",")\n","\n","\n","valid_data_loader = DataLoader(\n","    valid_dataset,\n","    batch_size=batch_size,\n","    shuffle=True,\n","    num_workers=0,\n","    collate_fn=collate_fn\n",")"]},{"cell_type":"markdown","metadata":{"id":"EJxFzhEnWugZ"},"source":["### Number of total batches"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UfnIbFjxWugZ","outputId":"022d861b-5fcf-481f-d827-fd3c74d8f1db"},"outputs":[{"name":"stdout","output_type":"stream","text":["2897\n","333\n"]}],"source":["train_total_batch = len(train_data_loader)\n","print(train_total_batch)\n","test_batch = len(valid_data_loader)\n","print(test_batch)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Df48J3wKWugZ"},"outputs":[],"source":["lr = 0.001\n","params = [p for p in resnet_cnn.parameters() if p.requires_grad]\n","optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n","criterion = nn.CrossEntropyLoss()"]},{"cell_type":"markdown","metadata":{"id":"to_E-wTjWuga"},"source":["## Inference"]},{"cell_type":"markdown","metadata":{"id":"2bcLo8MZWuga"},"source":["### Image Inference"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pzUtD4QIWuga","outputId":"f8eb26c2-d589-4301-ab9d-30156d26171d"},"outputs":[{"data":{"text/plain":["'./car_data/cars-collage-1657824102.jpg'"]},"execution_count":30,"metadata":{},"output_type":"execute_result"}],"source":["'./car_data/cars-collage-1657824102.jpg'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9A6_rCQPWuga"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","from tqdm.notebook import tqdm\n","import torch\n","import torchvision\n","import torchvision.transforms as T\n","from collections import defaultdict, deque\n","from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n","from torchvision.models.detection import FasterRCNN\n","from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n","from torchvision.models.detection import FasterRCNN\n","from torchvision.models.detection.rpn import AnchorGenerator\n","import ast\n","from torch.utils.data import DataLoader, Dataset\n","from torch.utils.data.sampler import SequentialSampler\n","import torchvision.transforms as transforms\n","import cv2\n","import os,sys,matplotlib,re\n","from PIL import Image\n","from skimage import exposure\n","import matplotlib.pyplot as plt\n","import matplotlib.image as immg\n","from torchvision.io import read_image\n","import matplotlib\n","from torchvision.utils import draw_bounding_boxes\n","import math\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aFVTjX2hdUbm"},"outputs":[{"data":{"text/plain":["device(type='cpu')"]},"execution_count":32,"metadata":{},"output_type":"execute_result"}],"source":["device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","device"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["d07e5f4403f3448dac84b3a4bd4712c3","e6df819945934a86a94c862b4570caa1","6eb1f7de2a6646beba6cdebd71431cf2","818348dd6f544219a5dfe3e4ae47b3b0","23784341a5934c4b9480f9691b19f121","3bd46d11b1f64ff4809d7da95dc2ef40","4ac7b6d64985498d8a38aa73d0ee7966","4cb83d7a6e6440678682204bc3c3e4ee","37951663a39e477393c51f7aa741c9c3","d452ccbfd9eb447190645595f8c44ef1","45ef09af122245a29b157b91749dc8b7"]},"executionInfo":{"elapsed":5085,"status":"ok","timestamp":1669356271754,"user":{"displayName":"Divyesh Patel","userId":"04224605867503865569"},"user_tz":-330},"id":"4gHsOBTjWugb","outputId":"8ff31149-08a7-4d14-89fb-680e6ec14f3b"},"outputs":[],"source":["# Load\n","model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n","in_features = model.roi_heads.box_predictor.cls_score.in_features\n","model.roi_heads.box_predictor = FastRCNNPredictor(in_features, 2)\n","model.load_state_dict(torch.load(\"./FL_fasterrcnn_car_detect.pth\", map_location=torch.device(device)))\n","model.eval()\n","# model.load_state_dict(torch.load(\"./FL_fasterrcnn_car_detect.pth\", map_location=torch.device('cpu')))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4OH_dPIxWugb"},"outputs":[],"source":["# Function to Detect Cars in an Image\n","def detect(img_path):\n","\n","    image = cv2.imread(img_path,cv2.IMREAD_COLOR)\n","    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n","    image /= 255.0\n","    image = np.transpose(image, (2, 0, 1))\n","    image_tensor = torch.from_numpy(image)\n","\n","    with torch.no_grad():\n","        prediction = model([image_tensor.to(device)])[0]\n","\n","    keep = torchvision.ops.nms(prediction['boxes'], prediction['scores'], 0.7)\n","    \n","    final_prediction = prediction\n","    final_prediction['boxes'] = final_prediction['boxes'][keep]\n","    final_prediction['scores'] = final_prediction['scores'][keep]\n","    final_prediction['labels'] = final_prediction['labels'][keep]\n","\n","    fig = plt.figure(figsize=(14, 10))\n","    img_int = torch.tensor(image_tensor * 255, dtype=torch.uint8)\n","    pred_img = draw_bounding_boxes(img_int, final_prediction['boxes'], width=4, colors=(255, 0, 0)).permute(1, 2, 0)\n","    plt.imshow(pred_img)\n","\n","    return pred_img"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":446},"executionInfo":{"elapsed":12233,"status":"ok","timestamp":1669356292941,"user":{"displayName":"Divyesh Patel","userId":"04224605867503865569"},"user_tz":-330},"id":"1PCEaFw_Wugb","outputId":"3a61573e-2932-4148-eb87-a74f7ebb2ae9"},"outputs":[],"source":["pred_img = detect(\"/content/drive/MyDrive/Federated_Learning/traffic_image.png\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qg-NmZ3bWugb","outputId":"4b05a7cd-fba5-423e-b500-cbeff3f8b821"},"outputs":[],"source":["pred_img = detect(\"./car_data/cars-collage-1657824102.jpg\")"]},{"cell_type":"markdown","metadata":{"id":"ThcpV5_bWugb"},"source":["### Video Inference"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bkZGtyqEWugb"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","from tqdm.notebook import tqdm\n","import torch\n","import torchvision\n","import torchvision.transforms as T\n","from collections import defaultdict, deque\n","from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n","from torchvision.models.detection import FasterRCNN\n","from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n","from torchvision.models.detection import FasterRCNN\n","from torchvision.models.detection.rpn import AnchorGenerator\n","import ast\n","from torch.utils.data import DataLoader, Dataset\n","from torch.utils.data.sampler import SequentialSampler\n","import torchvision.transforms as transforms\n","import cv2\n","import os,sys,matplotlib,re\n","from PIL import Image\n","from skimage import exposure\n","import matplotlib.pyplot as plt\n","import matplotlib.image as immg\n","from torchvision.io import read_image\n","import matplotlib\n","from torchvision.utils import draw_bounding_boxes\n","import math\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1669542016738,"user":{"displayName":"Divyesh Patel","userId":"04224605867503865569"},"user_tz":-330},"id":"DGyLTjchTh37","outputId":"e34e9a0e-dced-444b-8cfb-504afc700c11"},"outputs":[],"source":["device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","device"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2335,"status":"ok","timestamp":1669542064722,"user":{"displayName":"Divyesh Patel","userId":"04224605867503865569"},"user_tz":-330},"id":"qo1yaTSGWugb","outputId":"70d9f4ed-929e-4e58-e485-cf3510e3aef8"},"outputs":[],"source":["# Load\n","model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n","in_features = model.roi_heads.box_predictor.cls_score.in_features\n","model.roi_heads.box_predictor = FastRCNNPredictor(in_features, 2)\n","model.load_state_dict(torch.load(\"/content/drive/MyDrive/Federated_Learning/fasterrcnn_car_detect.pth\", map_location=torch.device(device)))\n","model.eval()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4gFqcPJPWugc"},"outputs":[],"source":["# Function to Detect Cars in one frame of the video\n","def detect_frame(frame):\n","    \n","    image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB).astype(np.float32)\n","    image /= 255.0\n","    image = np.transpose(image, (2, 0, 1))\n","    image_tensor = torch.from_numpy(image)\n","\n","    with torch.no_grad():\n","        model.to(device)\n","        prediction = model([image_tensor.to(device)])[0]\n","\n","    keep = torchvision.ops.nms(prediction['boxes'], prediction['scores'], 0.7)\n","\n","    final_prediction = prediction\n","    final_prediction['boxes'] = final_prediction['boxes'][keep]\n","    final_prediction['scores'] = final_prediction['scores'][keep]\n","    final_prediction['labels'] = final_prediction['labels'][keep]\n","\n","    # fig = plt.figure(figsize=(14, 10))\n","    img_int = torch.tensor(image_tensor * 255, dtype=torch.uint8)\n","    pred_img = draw_bounding_boxes(img_int, final_prediction['boxes'], width=4, colors=(255, 0, 0)).permute(1, 2, 0)\n","    # plt.imshow(pred_img)\n","\n","    return pred_img"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U9OY13C1ixFc"},"outputs":[],"source":["# Function to check number of frames in a video\n","def check_frames(video_path):\n","\n","    cap = cv2.VideoCapture(video_path)\n","    frame_no = 0\n","    while True:\n","        ret, frame = cap.read()\n","\n","        if ret == True:\n","            frame_no += 1\n","        else:\n","            break\n","\n","    print(\"Total number of frames in video: \",frame_no)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":469,"status":"ok","timestamp":1669542106598,"user":{"displayName":"Divyesh Patel","userId":"04224605867503865569"},"user_tz":-330},"id":"Q20Qt9Nbi95V","outputId":"17fe5eed-4e93-4791-ded3-d875b647f3d1"},"outputs":[],"source":["check_frames(\"/content/drive/MyDrive/Federated_Learning/Traffic_5_sec.mp4\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7Euund71iN3G"},"outputs":[],"source":["# Function to detect cars in a video\n","def detect_video(video_path, save_path=\"result.avi\"):\n","    cap = cv2.VideoCapture(video_path)\n","\n","    frame_width = int(cap.get(3))\n","    frame_height = int(cap.get(4))\n","    \n","    size = (frame_width, frame_height)\n","\n","    result = cv2.VideoWriter(save_path, cv2.VideoWriter_fourcc(*'MJPG'), 10, size)\n","\n","    if (cap.isOpened() == False): \n","        print(\"Error reading video file\")\n","    \n","    frame_no = 0\n","    while True:\n","        ret, frame = cap.read()\n","        if ret == True:\n","            output_frame = detect_frame(frame)\n","            numpy_frame = np.array(output_frame)\n","            result.write(numpy_frame)\n","        else:\n","            break\n","        frame_no += 1\n","\n","    cap.release()\n","    result.release()\n","\n","    print(\"Output Video is saved Successfully!!!\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":410792,"status":"ok","timestamp":1669542581773,"user":{"displayName":"Divyesh Patel","userId":"04224605867503865569"},"user_tz":-330},"id":"-gYw5vz0jFAs","outputId":"a5f07874-6537-49db-838e-ba16646fab9b"},"outputs":[],"source":["detect_video(\"/content/drive/MyDrive/Federated_Learning/Traffic_2_sec.mp4\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U1n5MlpNGhN9"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3.10.8 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8"},"vscode":{"interpreter":{"hash":"9321091b4504c222f29c630232bebbbfda669e2e4d0f632bfa597d159cf9e956"}},"widgets":{"application/vnd.jupyter.widget-state+json":{"23784341a5934c4b9480f9691b19f121":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"37951663a39e477393c51f7aa741c9c3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3bd46d11b1f64ff4809d7da95dc2ef40":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"45ef09af122245a29b157b91749dc8b7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4ac7b6d64985498d8a38aa73d0ee7966":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4cb83d7a6e6440678682204bc3c3e4ee":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6eb1f7de2a6646beba6cdebd71431cf2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4cb83d7a6e6440678682204bc3c3e4ee","max":167502836,"min":0,"orientation":"horizontal","style":"IPY_MODEL_37951663a39e477393c51f7aa741c9c3","value":167502836}},"818348dd6f544219a5dfe3e4ae47b3b0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d452ccbfd9eb447190645595f8c44ef1","placeholder":"​","style":"IPY_MODEL_45ef09af122245a29b157b91749dc8b7","value":" 160M/160M [00:02&lt;00:00, 77.1MB/s]"}},"d07e5f4403f3448dac84b3a4bd4712c3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e6df819945934a86a94c862b4570caa1","IPY_MODEL_6eb1f7de2a6646beba6cdebd71431cf2","IPY_MODEL_818348dd6f544219a5dfe3e4ae47b3b0"],"layout":"IPY_MODEL_23784341a5934c4b9480f9691b19f121"}},"d452ccbfd9eb447190645595f8c44ef1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e6df819945934a86a94c862b4570caa1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3bd46d11b1f64ff4809d7da95dc2ef40","placeholder":"​","style":"IPY_MODEL_4ac7b6d64985498d8a38aa73d0ee7966","value":"100%"}}}}},"nbformat":4,"nbformat_minor":0}
