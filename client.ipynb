{"cells":[{"cell_type":"markdown","metadata":{"id":"7eFtpqIwHuMC"},"source":["# Object Detection Federated CNN Client Side\n","This code is the Client part of Car Detection federated CNN model for **multi** client and a server."]},{"cell_type":"code","execution_count":1,"metadata":{"id":"_fWdZiXIHuMF"},"outputs":[],"source":["import os\n","import sys\n","import time\n","import copy\n","import h5py\n","import struct\n","import socket\n","import pickle\n","from tqdm import tqdm\n","\n","import torch\n","import torch.nn as nn\n","from threading import Lock\n","import torch.optim as optim\n","from threading import Thread\n","import torch.nn.functional as F"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"t8E0p-eoHuMF"},"outputs":[],"source":["import cv2\n","import ast\n","import math\n","import matplotlib\n","import torchvision\n","import numpy as np\n","import pandas as pd\n","from PIL import Image\n","import os,sys,matplotlib,re\n","from skimage import exposure\n","from tqdm.notebook import tqdm\n","import matplotlib.pyplot as plt\n","import matplotlib.image as immg\n","import torchvision.transforms as T\n","from torchvision.io import read_image\n","from collections import defaultdict, deque\n","import torchvision.transforms as transforms\n","from torch.utils.data import DataLoader, Dataset\n","from torchvision.utils import draw_bounding_boxes\n","from torchvision.models.detection import FasterRCNN\n","from torch.utils.data.sampler import SequentialSampler\n","from torchvision.models.detection.rpn import AnchorGenerator\n","from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["import torch"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["# !pip install opencv"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"Ozk32U9YHuMG"},"outputs":[],"source":["root_path = '../../models/'"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"soPEI0R_HuME"},"outputs":[],"source":["users = 2 # number of clients"]},{"cell_type":"markdown","metadata":{"id":"9N_Bl_XbHuMG"},"source":["## Declaring Device"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"WBl2buTnHuMG","outputId":"8d0754c2-e515-4c6d-8a1d-026e4f473014"},"outputs":[{"data":{"text/plain":["device(type='cpu')"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["# Checking if cuda is available \n","device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","device"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"X7PjsFlbHuMH"},"outputs":[],"source":["# Give client order\n","client_order = int(input(\"client_order(start from 0): \"))"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"ayYkgIIuHuMH"},"outputs":[],"source":["# Dividing Dataset depending on number of clients\n","num_traindata = 13244 // users"]},{"cell_type":"markdown","metadata":{"id":"5Nbtm-NIHuMH"},"source":["## Data load"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"4aimR8cDHuMH"},"outputs":[],"source":["#!wget --header=\"Host: storage.googleapis.com\" --header=\"User-Agent: Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/107.0.0.0 Safari/537.36\" --header=\"Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\" --header=\"Accept-Language: en-GB,en-US;q=0.9,en;q=0.8\" --header=\"Referer: https://www.kaggle.com/\" --header=\"Cookie: _ga=GA1.3.335780279.1668755863\" --header=\"Connection: keep-alive\" \"https://storage.googleapis.com/kaggle-data-sets/2377760/4010797/compressed/car_data.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20221121%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20221121T074631Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=454b8b6729685c0d2365ffe1277cf83c5a24ff9d9cf52ead18b60bd6287b1262d6559a5d26592f5ce34ad70afcafe9bee558e49b80fcb0a02630f0266b38ba631530024c3d98aead4fb8ceaf2d3dabde61fa1d896e8667a4c198f0583336c3e8864c4e92429385647353046534a8651f57404c1ba3854540a3ad8752c1a982a7027c154dc51cc7b09584b667ae2d2e06d05abbbc1ed4121ceb881cd25c7d29c3e56853fb82219cce8decca0b3ff4deae87ce1c89be997f23671c1b1e8b39fc4ce0a9db69ade5fd48bb58761f6bdee3b577ce960406410425104c0e0745f262ecfe1f4b147f503f2f1ed7868c98700b89969468475a6d4ca15b9b91919c308932\" -c -O 'car_data.zip'"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"egsHlgjkHuMI"},"outputs":[],"source":["#!unzip \"./car_data.zip\""]},{"cell_type":"code","execution_count":12,"metadata":{"id":"hG5rBAz8HuMI"},"outputs":[],"source":["# Dataset path\n","bbox_path = './car_data/images_w_boxes.csv'\n","image_folder_path = './car_data/images/'"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"7tk14M0JHuMI","outputId":"e7b57e96-3718-4cfc-8dae-ac1630894d06"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>img_path</th>\n","      <th>bbox_x1</th>\n","      <th>bbox_y1</th>\n","      <th>bbox_x2</th>\n","      <th>bbox_y2</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>bd3138e00c925841a3934f3075f9c734.jpg</td>\n","      <td>412</td>\n","      <td>28</td>\n","      <td>745</td>\n","      <td>185</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>7b9bf41480eafc0c11d2e56502a93b45.jpg</td>\n","      <td>250</td>\n","      <td>229</td>\n","      <td>860</td>\n","      <td>634</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>ca56229ef626d9e0e24a496c5e490018.jpg</td>\n","      <td>361</td>\n","      <td>231</td>\n","      <td>966</td>\n","      <td>628</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>308f316edd39e74f7636fc8a40eeb712.jpg</td>\n","      <td>151</td>\n","      <td>230</td>\n","      <td>925</td>\n","      <td>655</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>bc7b5c4b60a7bcd4e477755b64c69b34.jpg</td>\n","      <td>35</td>\n","      <td>219</td>\n","      <td>939</td>\n","      <td>657</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>6453</th>\n","      <td>910b7b2b480f9df5751ef22206e79532.jpg</td>\n","      <td>162</td>\n","      <td>127</td>\n","      <td>830</td>\n","      <td>563</td>\n","    </tr>\n","    <tr>\n","      <th>6454</th>\n","      <td>6ebab0ac8470e8614d9fa41ac6ff4121.jpg</td>\n","      <td>152</td>\n","      <td>614</td>\n","      <td>806</td>\n","      <td>900</td>\n","    </tr>\n","    <tr>\n","      <th>6455</th>\n","      <td>6d7bb960d07b7eecb3e875dca4c30605.jpg</td>\n","      <td>3</td>\n","      <td>294</td>\n","      <td>675</td>\n","      <td>908</td>\n","    </tr>\n","    <tr>\n","      <th>6456</th>\n","      <td>829d197c86a0363f9e31573f0312ff8b.jpg</td>\n","      <td>70</td>\n","      <td>132</td>\n","      <td>902</td>\n","      <td>512</td>\n","    </tr>\n","    <tr>\n","      <th>6457</th>\n","      <td>d77d36f2c00b2ff10ef70d5a71a440d5.jpg</td>\n","      <td>49</td>\n","      <td>121</td>\n","      <td>955</td>\n","      <td>567</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>6458 rows Ã— 5 columns</p>\n","</div>"],"text/plain":["                                  img_path  bbox_x1  bbox_y1  bbox_x2  bbox_y2\n","0     bd3138e00c925841a3934f3075f9c734.jpg      412       28      745      185\n","1     7b9bf41480eafc0c11d2e56502a93b45.jpg      250      229      860      634\n","2     ca56229ef626d9e0e24a496c5e490018.jpg      361      231      966      628\n","3     308f316edd39e74f7636fc8a40eeb712.jpg      151      230      925      655\n","4     bc7b5c4b60a7bcd4e477755b64c69b34.jpg       35      219      939      657\n","...                                    ...      ...      ...      ...      ...\n","6453  910b7b2b480f9df5751ef22206e79532.jpg      162      127      830      563\n","6454  6ebab0ac8470e8614d9fa41ac6ff4121.jpg      152      614      806      900\n","6455  6d7bb960d07b7eecb3e875dca4c30605.jpg        3      294      675      908\n","6456  829d197c86a0363f9e31573f0312ff8b.jpg       70      132      902      512\n","6457  d77d36f2c00b2ff10ef70d5a71a440d5.jpg       49      121      955      567\n","\n","[6458 rows x 5 columns]"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["# Reading csv\n","data_bbox = pd.read_csv(bbox_path)\n","data_bbox"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"B1y2rYiVHuMI"},"outputs":[],"source":["df = data_bbox.copy()"]},{"cell_type":"markdown","metadata":{"id":"_s_x81T7caT2"},"source":["## Defining Car Dataset Class\n"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"2_HtFGPkHuMI"},"outputs":[],"source":["class CarDataset(object):\n","    def __init__(self, df, IMG_DIR, transforms=None):\n","        self.a = 0\n","        self.df = df\n","        self.img_dir = IMG_DIR\n","        self.image_ids = self.df['img_path'].unique().tolist()\n","        self.transforms = transforms\n","        \n","    def __len__(self):\n","        return len(self.image_ids)\n","        \n","    def __getitem__(self, idx):\n","        image_id = self.image_ids[idx]\n","        records = self.df[self.df['img_path'] == image_id]\n","        image = cv2.imread(self.img_dir+image_id,cv2.IMREAD_COLOR)\n","        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n","        image /= 255.0\n","\n","        boxes = records[['bbox_x1', 'bbox_y1', 'bbox_x2', 'bbox_y2']].to_numpy()\n","        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n","        labels = torch.ones((records.shape[0],), dtype=torch.int64)\n","        \n","        target = {}\n","        target['boxes'] = boxes\n","        target['labels'] = labels\n","        target['image_id'] = torch.tensor([idx])\n","        target['area'] = torch.as_tensor(area, dtype=torch.float32)\n","        target['iscrowd'] = torch.zeros((records.shape[0],), dtype=torch.int64)\n","    \n","        if self.transforms:\n","            sample = {\n","                'image': image,\n","                'bboxes': target['boxes'],\n","                'labels': labels\n","            }\n","            sample = self.transforms(**sample)\n","            image = sample['image']\n","            \n","            target['boxes'] = torch.stack(tuple(map(torch.tensor, zip(*sample['bboxes'])))).permute(1, 0)\n","        return image.clone().detach(), target, image_id"]},{"cell_type":"markdown","metadata":{"id":"qCrOVScvHuMJ"},"source":["## Making Batch Generator"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"vBO3phBXHuMJ"},"outputs":[],"source":["batch_size = 32"]},{"cell_type":"markdown","metadata":{"id":"_euRtbQCdGHU"},"source":["### Data Augmentation"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"NjOTy3rAHuMJ"},"outputs":[],"source":["import albumentations as A\n","from albumentations.pytorch.transforms import ToTensorV2\n","def train_transform():\n","    return A.Compose([\n","        A.Flip(0.5),\n","        ToTensorV2()\n","    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n","\n","def valid_transform():\n","    return A.Compose([\n","        ToTensorV2()\n","    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"X_74jwMaHuMJ","outputId":"f8839035-dc87-4755-b274-6717b5aa21be"},"outputs":[{"data":{"text/plain":["((5793, 5), (665, 5))"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["image_ids = df['img_path'].unique()\n","valid_ids = image_ids[-665:]\n","train_ids = image_ids[:-665]\n","valid_df = df[df['img_path'].isin(valid_ids)]\n","train_df = df[df['img_path'].isin(train_ids)]\n","train_df.shape,valid_df.shape"]},{"cell_type":"markdown","metadata":{"id":"09itwxIeHuMJ"},"source":["### `DataLoader` for batch generating\n"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"W24waSu6HuMK"},"outputs":[],"source":["def collate_fn(batch):\n","    return tuple(zip(*batch))\n","\n","train_dataset = CarDataset(train_df, image_folder_path, train_transform())\n","valid_dataset = CarDataset(valid_df, image_folder_path, valid_transform())\n","\n","indices = torch.randperm(len(train_dataset)).tolist()\n","train_data_loader = DataLoader(\n","    train_dataset,\n","    batch_size=batch_size,\n","    shuffle=True,\n","    num_workers=0,\n","    collate_fn=collate_fn\n",")\n","\n","\n","valid_data_loader = DataLoader(\n","    valid_dataset,\n","    batch_size=batch_size,\n","    shuffle=True,\n","    num_workers=0,\n","    collate_fn=collate_fn\n",")"]},{"cell_type":"markdown","metadata":{"id":"ZaLkZagvHuMK"},"source":["### Number of total batches"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"WGtQr4IUHuMK","outputId":"14cbebbe-e186-4d5d-8df7-2d7a17d16b0f"},"outputs":[{"name":"stdout","output_type":"stream","text":["91\n"]}],"source":["train_total_batch = len(train_data_loader)\n","print(train_total_batch)\n","# test_batch = len(testloader)\n","# print(test_batch)"]},{"cell_type":"markdown","metadata":{"id":"Xj-SwcRvdOf9"},"source":["## CNN Model for object Detection"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"_IyOen2WHuMK"},"outputs":[],"source":["# Defining Pytorch CNN pretrained model\n","def model1(num):\n","    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n","    in_features = model.roi_heads.box_predictor.cls_score.in_features\n","    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num)\n","    return model"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"919BBOkZHuMK"},"outputs":[],"source":["# Creating object of model\n","resnet_cnn = model1(2)"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"6mqvHIxrHuML","outputId":"5d517c0d-769f-4287-911a-5c3992ccc1c7"},"outputs":[{"data":{"text/plain":["FasterRCNN(\n","  (transform): GeneralizedRCNNTransform(\n","      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n","  )\n","  (backbone): BackboneWithFPN(\n","    (body): IntermediateLayerGetter(\n","      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","      (bn1): FrozenBatchNorm2d(64, eps=0.0)\n","      (relu): ReLU(inplace=True)\n","      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","      (layer1): Sequential(\n","        (0): Bottleneck(\n","          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n","          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n","          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","          (downsample): Sequential(\n","            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","            (1): FrozenBatchNorm2d(256, eps=0.0)\n","          )\n","        )\n","        (1): Bottleneck(\n","          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n","          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n","          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (2): Bottleneck(\n","          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n","          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n","          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","      )\n","      (layer2): Sequential(\n","        (0): Bottleneck(\n","          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n","          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","          (downsample): Sequential(\n","            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","            (1): FrozenBatchNorm2d(512, eps=0.0)\n","          )\n","        )\n","        (1): Bottleneck(\n","          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n","          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (2): Bottleneck(\n","          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n","          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (3): Bottleneck(\n","          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n","          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","      )\n","      (layer3): Sequential(\n","        (0): Bottleneck(\n","          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","          (downsample): Sequential(\n","            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","            (1): FrozenBatchNorm2d(1024, eps=0.0)\n","          )\n","        )\n","        (1): Bottleneck(\n","          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (2): Bottleneck(\n","          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (3): Bottleneck(\n","          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (4): Bottleneck(\n","          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (5): Bottleneck(\n","          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","      )\n","      (layer4): Sequential(\n","        (0): Bottleneck(\n","          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n","          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n","          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","          (downsample): Sequential(\n","            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","            (1): FrozenBatchNorm2d(2048, eps=0.0)\n","          )\n","        )\n","        (1): Bottleneck(\n","          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n","          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n","          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (2): Bottleneck(\n","          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n","          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n","          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","      )\n","    )\n","    (fpn): FeaturePyramidNetwork(\n","      (inner_blocks): ModuleList(\n","        (0): Conv2dNormActivation(\n","          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n","        )\n","        (1): Conv2dNormActivation(\n","          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n","        )\n","        (2): Conv2dNormActivation(\n","          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n","        )\n","        (3): Conv2dNormActivation(\n","          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n","        )\n","      )\n","      (layer_blocks): ModuleList(\n","        (0): Conv2dNormActivation(\n","          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        )\n","        (1): Conv2dNormActivation(\n","          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        )\n","        (2): Conv2dNormActivation(\n","          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        )\n","        (3): Conv2dNormActivation(\n","          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        )\n","      )\n","      (extra_blocks): LastLevelMaxPool()\n","    )\n","  )\n","  (rpn): RegionProposalNetwork(\n","    (anchor_generator): AnchorGenerator()\n","    (head): RPNHead(\n","      (conv): Sequential(\n","        (0): Conv2dNormActivation(\n","          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","          (1): ReLU(inplace=True)\n","        )\n","      )\n","      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n","      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n","    )\n","  )\n","  (roi_heads): RoIHeads(\n","    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n","    (box_head): TwoMLPHead(\n","      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n","      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n","    )\n","    (box_predictor): FastRCNNPredictor(\n","      (cls_score): Linear(in_features=1024, out_features=2, bias=True)\n","      (bbox_pred): Linear(in_features=1024, out_features=8, bias=True)\n","    )\n","  )\n",")"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["# Sending device to device(cpu or gpu)\n","resnet_cnn.to(device)"]},{"cell_type":"code","execution_count":24,"metadata":{"id":"Hp3P1kASHuML"},"outputs":[],"source":["import torch.nn as nn\n","criterion = nn.CrossEntropyLoss()\n","rounds = 3 # default\n","local_epochs = 1 # default\n","lr = 0.001\n","# optimizer = Adam(resnet_cnn.parameters(), lr=lr)\n","params = [p for p in resnet_cnn.parameters() if p.requires_grad]\n","optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n"]},{"cell_type":"markdown","metadata":{"id":"yCMUV6S6HuML"},"source":["## Socket initialization\n","### Required socket functions"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"kBROcGwdHuML"},"outputs":[],"source":["def RECVALL(sock, n):\n","    # helper function to receive n bytes or return None if EOF is hit\n","    print(\"11111\")\n","    data = b''\n","    while len(data) < n:\n","        print(\"22222\")\n","        packet = sock.recv(n - len(data))\n","        print(\"33333\")\n","        if not packet:\n","            return None\n","        print(\"44444\")\n","        data += packet\n","    return data\n","    \n","def SEND_MSG(sock, msg):\n","    # prefix each message with a 4-byte length in network byte order\n","    msg = pickle.dumps(msg)\n","    msg = struct.pack('>I', len(msg)) + msg\n","    sock.sendall(msg)\n","\n","def RECV_MSG(sock):\n","    # read message length and unpack it into an integer\n","    raw_msglen = RECVALL(sock, 4)\n","    if not raw_msglen:\n","        return None\n","    msglen = struct.unpack('>I', raw_msglen)[0]\n","    # read the message data\n","    msg =  RECVALL(sock, msglen)\n","    msg = pickle.loads(msg)\n","    return msg"]},{"cell_type":"markdown","metadata":{"id":"5E3lakKqHuML"},"source":["### Set host address and port number"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"8Ofi0NkMHuML"},"outputs":[],"source":["host = input(\"IP address: \")\n","port = 10087\n","max_recv = 100000"]},{"cell_type":"markdown","metadata":{"id":"imGXzwa3HuML"},"source":["### Open the client socket"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[],"source":["from socket import *\n","import socket"]},{"cell_type":"code","execution_count":28,"metadata":{"id":"wJD0TmYqHuML"},"outputs":[],"source":["s = socket.socket()\n","s.connect((host, port))"]},{"cell_type":"markdown","metadata":{"id":"BBRA2K6sHuMM"},"source":["## SET TIMER"]},{"cell_type":"code","execution_count":29,"metadata":{"id":"VAFSeNjmHuMM","outputId":"b14056d0-87f3-4de2-aeb0-f390542f2a54"},"outputs":[{"name":"stdout","output_type":"stream","text":["timmer start!\n"]}],"source":["import time \n","start_time = time.time()    # store start time\n","print(\"timmer start!\")"]},{"cell_type":"markdown","metadata":{"id":"WiJod3MMeLHj"},"source":["### Receiving Details like number of epochs from server"]},{"cell_type":"code","execution_count":30,"metadata":{"id":"Y8ddgI7LHuMM","outputId":"12f66493-113b-40a0-82c5-4bc498e133a3"},"outputs":[{"name":"stdout","output_type":"stream","text":["11111\n","22222\n","33333\n","44444\n","11111\n","22222\n","33333\n","44444\n"]}],"source":["msg = RECV_MSG(s)\n","rounds = msg['rounds']\n","client_id = msg['client_id']\n","local_epochs = msg['local_epoch']\n","SEND_MSG(s, len(train_dataset))"]},{"cell_type":"markdown","metadata":{"id":"sHWdawnCeYFW"},"source":["### Training"]},{"cell_type":"code","execution_count":31,"metadata":{"id":"L4ROjsKDHuMM"},"outputs":[{"name":"stdout","output_type":"stream","text":["11111\n","22222\n","33333\n","44444\n","11111\n","22222\n","33333\n","44444\n","22222\n","33333\n","44444\n","22222\n","33333\n","44444\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5deb6a8ea74749e8b54b391860e614d0","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/91 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"ename":"RuntimeError","evalue":"[enforce fail at ..\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 1497366528 bytes.","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[1;32mIn [31], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m images \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(image\u001b[39m.\u001b[39mto(device) \u001b[39mfor\u001b[39;00m image \u001b[39min\u001b[39;00m images)\n\u001b[0;32m     13\u001b[0m targets \u001b[39m=\u001b[39m [{k: torch\u001b[39m.\u001b[39mtensor(v)\u001b[39m.\u001b[39mto(device) \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m t\u001b[39m.\u001b[39mitems()} \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m targets]\n\u001b[1;32m---> 15\u001b[0m loss_dict \u001b[39m=\u001b[39m resnet_cnn(images, targets) \n\u001b[0;32m     16\u001b[0m losses \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(loss \u001b[39mfor\u001b[39;00m loss \u001b[39min\u001b[39;00m loss_dict\u001b[39m.\u001b[39mvalues())\n\u001b[0;32m     17\u001b[0m loss_dict_append \u001b[39m=\u001b[39m {k: v\u001b[39m.\u001b[39mitem() \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m loss_dict\u001b[39m.\u001b[39mitems()}\n","File \u001b[1;32mc:\\Users\\karan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[1;32mc:\\Users\\karan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\detection\\generalized_rcnn.py:101\u001b[0m, in \u001b[0;36mGeneralizedRCNN.forward\u001b[1;34m(self, images, targets)\u001b[0m\n\u001b[0;32m     94\u001b[0m             degen_bb: List[\u001b[39mfloat\u001b[39m] \u001b[39m=\u001b[39m boxes[bb_idx]\u001b[39m.\u001b[39mtolist()\n\u001b[0;32m     95\u001b[0m             torch\u001b[39m.\u001b[39m_assert(\n\u001b[0;32m     96\u001b[0m                 \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m     97\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mAll bounding boxes should have positive height and width.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     98\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m Found invalid box \u001b[39m\u001b[39m{\u001b[39;00mdegen_bb\u001b[39m}\u001b[39;00m\u001b[39m for target at index \u001b[39m\u001b[39m{\u001b[39;00mtarget_idx\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     99\u001b[0m             )\n\u001b[1;32m--> 101\u001b[0m features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbackbone(images\u001b[39m.\u001b[39;49mtensors)\n\u001b[0;32m    102\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(features, torch\u001b[39m.\u001b[39mTensor):\n\u001b[0;32m    103\u001b[0m     features \u001b[39m=\u001b[39m OrderedDict([(\u001b[39m\"\u001b[39m\u001b[39m0\u001b[39m\u001b[39m\"\u001b[39m, features)])\n","File \u001b[1;32mc:\\Users\\karan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[1;32mc:\\Users\\karan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\detection\\backbone_utils.py:58\u001b[0m, in \u001b[0;36mBackboneWithFPN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, Tensor]:\n\u001b[0;32m     57\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbody(x)\n\u001b[1;32m---> 58\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfpn(x)\n\u001b[0;32m     59\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n","File \u001b[1;32mc:\\Users\\karan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[1;32mc:\\Users\\karan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\ops\\feature_pyramid_network.py:196\u001b[0m, in \u001b[0;36mFeaturePyramidNetwork.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    194\u001b[0m     inner_top_down \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39minterpolate(last_inner, size\u001b[39m=\u001b[39mfeat_shape, mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mnearest\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    195\u001b[0m     last_inner \u001b[39m=\u001b[39m inner_lateral \u001b[39m+\u001b[39m inner_top_down\n\u001b[1;32m--> 196\u001b[0m     results\u001b[39m.\u001b[39minsert(\u001b[39m0\u001b[39m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_result_from_layer_blocks(last_inner, idx))\n\u001b[0;32m    198\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mextra_blocks \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    199\u001b[0m     results, names \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mextra_blocks(results, x, names)\n","File \u001b[1;32mc:\\Users\\karan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\ops\\feature_pyramid_network.py:169\u001b[0m, in \u001b[0;36mFeaturePyramidNetwork.get_result_from_layer_blocks\u001b[1;34m(self, x, idx)\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[39mfor\u001b[39;00m i, module \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer_blocks):\n\u001b[0;32m    168\u001b[0m     \u001b[39mif\u001b[39;00m i \u001b[39m==\u001b[39m idx:\n\u001b[1;32m--> 169\u001b[0m         out \u001b[39m=\u001b[39m module(x)\n\u001b[0;32m    170\u001b[0m \u001b[39mreturn\u001b[39;00m out\n","File \u001b[1;32mc:\\Users\\karan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[1;32mc:\\Users\\karan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    203\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 204\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    205\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n","File \u001b[1;32mc:\\Users\\karan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[1;32mc:\\Users\\karan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n","File \u001b[1;32mc:\\Users\\karan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[0;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[0;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[1;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[0;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n","\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at ..\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 1497366528 bytes."]}],"source":["lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n","for epoch in range(rounds):\n","    weights = RECV_MSG(s)\n","    resnet_cnn.load_state_dict(weights)\n","    resnet_cnn.eval()\n","    loss_num = 0\n","    resnet_cnn.train()\n","    all_losses = []\n","    all_losses_dict = []\n","    \n","    for images, targets, _ in tqdm(train_data_loader):\n","        images = list(image.to(device) for image in images)\n","        targets = [{k: torch.tensor(v).to(device) for k, v in t.items()} for t in targets]\n","        \n","        loss_dict = resnet_cnn(images, targets) \n","        losses = sum(loss for loss in loss_dict.values())\n","        loss_dict_append = {k: v.item() for k, v in loss_dict.items()}\n","        loss_value = losses.item()\n","        \n","        all_losses.append(loss_value)\n","        all_losses_dict.append(loss_dict_append)\n","        \n","        if not math.isfinite(loss_value):\n","            print(f\"Loss is {loss_value}, stopping trainig\") \n","            print(loss_dict)\n","            sys.exit(1)\n","        \n","        optimizer.zero_grad()\n","        losses.backward()\n","        optimizer.step()\n","        if lr_scheduler is not None:\n","            lr_scheduler.step()\n","\n","    all_losses_dict = pd.DataFrame(all_losses_dict)\n","    print(\"Epoch {}, lr: {:.6f}, loss: {:.6f}, loss_classifier: {:.6f}, loss_box: {:.6f}, loss_rpn_box: {:.6f}, loss_object: {:.6f}\".format(\n","        epoch, optimizer.param_groups[0]['lr'], np.mean(all_losses),\n","        all_losses_dict['loss_classifier'].mean(),\n","        all_losses_dict['loss_box_reg'].mean(),\n","        all_losses_dict['loss_rpn_box_reg'].mean(),\n","        all_losses_dict['loss_objectness'].mean()\n","    ))\n","    \n","    msg = resnet_cnn.state_dict()\n","    SEND_MSG(s, msg)\n","\n","print('Finished Training')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-RIDFSLjHuMM","outputId":"86f9b76e-290b-4794-9224-416500f88461"},"outputs":[],"source":["end_time = time.time()  #store end time\n","print(\"Training Time: {} sec\".format(end_time - start_time))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3.10.8 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8"},"vscode":{"interpreter":{"hash":"9321091b4504c222f29c630232bebbbfda669e2e4d0f632bfa597d159cf9e956"}}},"nbformat":4,"nbformat_minor":0}
